---
title: "Exploring the CitiBike Data in New York"
output:
  prettydoc::html_pretty:
    theme: hpstr
    highlight: github
---

```{r, include = F}
rm(list = ls())

library(tidyverse)
library(lubridate)
library(ggmap)
library(data.table)
library(viridis)
library(sf)
library(units)
library(ggthemes)
library(parallel)
library(leaflet)
library(rjson)
library(mapview)
library(magick)
library(rprojroot)
library(widgetframe)

knitr::opts_chunk$set(echo = T, warning = F, message = F)
```

## Exploring the New York City CitiBike data

This project explores the CitiBike program data from New York City.

A few notes:

* The full project (Github repo) can be found [here](https://mrkaye97.github.io/CitiBike/).
* All R code is in `~/CitiBike/R`
* All python code is in `~/CitiBike/python`
* Some data used in certain files can be found in `~/CitiBike/data`, but much is pulled directly from online sources.

***

## Setup

The project requires no explicit setup. All data is either included in this repo or comes directly from the CitiBike website.

A handful of R and Python packages are needed to run the project. See the bottom of this README for a list.

*** 

## Data

The data comes from the [CitiBike program](https://www.citibikenyc.com/system-data) directly, and is publicly available for download.

```{r setup}
PROJECT_ROOT <- find_root('CitiBike.Rproj')
```

Below is a glimpse at the structure of the data

```{r}
fread(paste(PROJECT_ROOT, '/data/citibike_dat_1.csv', sep = ""), verbose = F, showProgress = F) %>%
  mutate(starttime = starttime %>% as_datetime(),
         stoptime = stoptime %>% as_datetime(),
         bikeid = bikeid %>% as.factor()) %>%
  sample_n(size = 100) %>%
  glimpse()
```

***

## Plots

#### Visualizing Commute Patterns -- Heatmap of start and end stations by time of day
<img src="https://raw.githubusercontent.com/mrkaye97/CitiBike/master/viz/commutes.svg">


#### Coronavirus Impact on Ridership

<img src="https://raw.githubusercontent.com/mrkaye97/CitiBike/master/viz/coronavirus-and-day-dsn.svg">

<br>

#### Common Routes

```{r}
df <- read.csv('https://raw.githubusercontent.com/mrkaye97/CitiBike/master/data/for_common_routes_leaflet.csv')

grouped_coords <- function(coord, group, order) {
  data.frame(coord = coord, group = group) %>%
    group_by(group) %>%
    purrrlyr::by_slice(~c(.$coord, NA), .to = "output") %>%
    left_join(data.frame(group = group, order = order) %>% distinct(), by = 'group') %>%
    arrange(order) %>%
    .$output %>%
    unlist()
}

pal <- colorFactor(
  palette = "magma", domain = NULL)

# Map using Leaflet R
l <- leaflet(df) %>%
  addProviderTiles("CartoDB.Positron") %>% 
  addPolylines(
    lng = ~grouped_coords(lon, routeid, rownames(df)),
    lat = ~grouped_coords(lat, routeid, rownames(df)),
    color = ~pal(df$numroute)) 
```

<center>
```{r, fig.width = 8, echo = F}
suppressWarnings(l)
```
</center>


We might also be interested in a long-term forecast of ridership. First, we inspect how ridership trends over time.

```{r, fig.height = 6, fig.width = 8}
rm(list = ls())

library(bsts)

load(url('https://github.com/mrkaye97/CitiBike/blob/master/R/ridership_bsts.Rdata?raw=true'))

df <- df %>%
  mutate(movavg = zoo::rollmean(numrides, 7, fill, align = 'center', fill = NA))
```


<center>
```{r, echo = F, dpi = 300}
df %>% 
  ggplot(aes(x = date))+
  geom_line(aes(y = numrides / 10**3), alpha = .4)+
  geom_line(aes(y = movavg / 10**3))+
  theme_fivethirtyeight()+
  labs(title = 'Total Rides by Date',
       caption = 'Dark black line is a 7-day moving average.\nPale gray line is the actual trend',
       y = 'Rides (Thousands)')+
  theme(axis.title.y = element_text())
```
</center>

Clearly, this series is highly volatile. We would expect there to be at least two seasonal components at play here: One for the time of the year (better weather in the summer means more bike rides) and one for day of the week (maybe there are more rides on weekdays than weekends because New Yorkers are using the bikes to commute). There is additionally quite likely to be an autoregressive compnent, since the number of rides today likely depends on the number of rides yesterday, and a trend component, since ridership might be increasing over time.

To model this time series, we turn to Bayesian Structural Time Series (BSTS), a technique that lets us decompose time series into its components, apply the Kalman Filter to estimate filtered values and project future values for the series, and do it all in a fully Bayesian paradigm. Consider the following plot, which decomposes the time series into its components.

<center>
```{r, fig.height = 10, fig.width=8, echo = F, dpi = 300}
burnin <- 1000
tibble(date = df$date, 
       "Actual Data" = forecast::tsclean(df$numrides),
       'AR' = colMeans(model1$state.contributions[-(1:burnin),"Ar1",]),
       "Daily Seasonality" = colMeans(model1$state.contributions[-(1:burnin),"seasonal.7.1",]),
       "Trend" = colMeans(model1$state.contributions[-(1:burnin),"trend",]),
       "Seasonality" = colMeans(model1$state.contributions[-(1:burnin),"trig.365", ])
       ) %>%
  pivot_longer(c(2:6), names_to = 'component', values_to = 'value') %>%
  ggplot(aes(x = date, y = value / 10**3, color = component))+
  geom_line()+
  facet_wrap(~component, scales = 'free', ncol = 1)+
  theme(legend.position = 'bottom', legend.direction = 'horizontal', axis.title.y = element_text())+
  theme_fivethirtyeight()+
  labs(y = "Rides (Thousands)", color = 'Component')
```
</center>

Here we see each of these components that we discussed separated out. In yellow, we have the true values of the observations. The purple and green lines are the two seasonal components: green accounting for the seasons in the year, and purple accounting for day of the week effects. The red is an autoregressive term, and the blue is the remainder -- the underlying trend.

Finally, we can use this BSTS model to predict out future ridership. The predictions from our model look like this:

```{r, echo = F, dpi = 300}
n <- 10
preds <- predict(model1, horizon = n)
new <- data.frame(date = seq(max(df$date), max(df$date) + n - 1, by = '1 day'),
                  numrides = preds$mean,
                  upper = preds$interval[2,],
                  lower = preds$interval[1,],
                  type = 'pred')

df %>%
  mutate(type = 'orig') %>%
  bind_rows(new) %>%
  filter(date > max(date) - 50) %>%
  ggplot(aes(date, numrides, color = type))+
  geom_line()+
  geom_ribbon(aes(ymax = upper, ymin = lower, alpha = .1), fill = 'gray', color = 'darkgray')

```

The predictions shown above are 10 steps (days) into the future. We might also be interested in a longer-term forecast. See the plot below for predictions over the next calendar year:

```{r, echo = F, dpi = 300}
n <- 365
preds <- predict(model1, horizon = n)
new <- data.frame(date = seq(max(df$date), max(df$date) + n - 1, by = '1 day'),
                  numrides = preds$mean,
                  upper = preds$interval[2,],
                  lower = preds$interval[1,],
                  type = 'pred')

df %>%
  mutate(type = 'orig') %>%
  bind_rows(new) %>%
  filter(date > max(date) - 750) %>%
  ggplot(aes(date, numrides, color = type))+
  geom_line()+
  geom_ribbon(aes(ymax = upper, ymin = lower, alpha = .1), fill = 'gray', color = 'darkgray')

```

Notice here that the trigonometric component shows up in our forecast (the curvature over longer time windows), as does the day of the week component (the jaggedness of the curve over shorter time windows), just as we would expect. 

In all, Bayesian Structural Time Series is an extremely powerful method of time series analysis, allowing us to decompose time series easily and forecast the values of the series into both the near and far future. One major advantage of the BSTS method over a more common ARIMA approach is that there is no need to ensure stationarity before fitting the model. In the BSTS approach, we can simply specify a trend component such that the current state is a function of the previous state times some rate of change, and achieve an increasing trend over time. In addition, the BSTS approach lets us determine filtered values of the series through the use of the Kalman Filter, which is particularly helpful in time series problems in which we are trying to understand the underlying "latent" process (state equation) when the data itself is noisy. This might come in handy when trying to predict the temperature inside of a rocket engine over time, for example, since the sensor recording the temperature data would report noisy measures since it would need to be outside of the engine itself, so as not to melt. Finally, BSTS is fully Bayesian, which means we can incorporate an arbitrary amount of prior information, and can get a true understanding of the posterior densities of our predictions and parameter estimates.

```{bash, include = F}
rm index.html
ln -s index.nb.html index.html
```

